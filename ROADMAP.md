# Roadmap ðŸ’£

## Phase 1: Foundation âœ… (Done â€” Feb 11, 2026)
- [x] VRM model loading + Three.js rendering
- [x] VRMA animation playback (163 animations)
- [x] Facial expressions (happy, sad, angry, surprised, relaxed)
- [x] Auto-blink + idle eye saccades
- [x] Mouse-follow look-at
- [x] WebSocket control API
- [x] Action state machine (Idle â†’ Action â†’ Speaking)
- [x] Idle micro-actions (random looking around, stretching, etc.)
- [x] Smooth crossfade between animations
- [x] Auto-load model on startup
- [x] UI control panel

## Phase 2: Voice & Chat âœ… (Done â€” Feb 11, 2026)
- [x] ElevenLabs TTS integration (server-side)
- [x] Audio-driven lip sync (Web Audio API AnalyserNode)
- [x] Voice input via Web Speech API
- [x] Chat UI overlay
- [x] OpenClaw agent integration (AI conversation)
- [x] Action/expression auto-selection based on response

## Phase 3: UI & Interaction âœ… (Done â€” Feb 11, 2026)
- [x] **Sakura/anime UI theme** â€” cute glassmorphism, pink palette, bubbly chat
- [x] **Modern chat bubbles** â€” user/avatar alignment, timestamps, ðŸ’£ avatar icon
- [x] **Cute controls** â€” emoji section headers, rounded pills, soft animations
- [x] **Beautiful drop prompt** â€” sparkle animation, kawaii styling

## Phase 4: Companion Features ðŸ”„ (Next â€” v0.2)
### High Priority
- [ ] **Touch reactions** â€” click/tap avatar for responses (headpat â†’ happy, poke â†’ surprised, etc.)
- [ ] **Quick emotion bar** â€” row of emoji buttons (ðŸ˜ŠðŸ˜¢ðŸ˜ ðŸ˜®ðŸ˜Œ) that trigger expression + animation combos
- [ ] **Background scenes** â€” selectable environments (sakura garden, cozy cafÃ©, night sky, starlit, warm sunset)
- [ ] **Camera presets** â€” quick buttons for face close-up, full body, portrait framing

### Medium Priority
- [ ] **Day/night cycle** â€” lighting & background shift based on real time (sunrise â†’ golden hour â†’ moonlit)
- [ ] **Photo mode** â€” screenshot button with cute frame/border, save as PNG
- [ ] **Greeting on load** â€” avatar waves and says "Welcome back~" with TTS on page open
- [ ] **Notification badge** â€” bouncing ðŸ’£ when avatar has something to say
- [ ] **Animation queue** â€” string together dance routines or action sequences

### Polish
- [ ] **Responsive/mobile layout** â€” touch-friendly for phones
- [ ] **Hide controls by default** â€” show on hover/tap, keep chat always visible
- [ ] **Idle breathing** â€” subtle body sway so avatar never looks frozen
- [ ] **Better animation filtering** â€” tag stationary vs root-motion animations

## Phase 4.5: iOS + Multimodal ðŸ”¥ (In Progress â€” v0.3)
### 1. Native iOS App (Capacitor)
- [ ] Install Capacitor, add iOS platform
- [ ] Build + sync web bundle to native iOS project
- [ ] Same experience as desktop â€” shared WS server for state sync
- [ ] Touch-optimized mobile layout
- [ ] Deploy to iPhone via Xcode

### 2. Online VRM Asset Shop
- [ ] Browse open-source VRM models (VRoid Hub, Booth.pm, etc.)
- [ ] In-app model browser UI â€” preview + one-click load
- [ ] Download and cache models locally

### 3. Streaming ASR + TTS + Multimodal Vision
- [ ] Real-time speech recognition (streaming ASR)
- [ ] Streaming TTS â€” start lip sync before full audio generated
- [ ] iPhone front camera integration â€” avatar can "see" the user
- [ ] Vision API analysis of camera frames (expressions, context)
- [ ] FaceTime-like experience: you see avatar, avatar sees you

### 4. Animation Refinement
- [ ] Fix root motion drift on Mixamo animations
- [ ] Smoother crossfade blending between actions
- [ ] Idle breathing / subtle body sway
- [ ] Tag stationary vs root-motion animations in catalog

## Phase 5: Platform & Advanced ðŸ“‹ (Planned â€” v0.4+)
- [ ] **PWA support** â€” manifest.json, service worker, installable on phone
- [ ] **OBS overlay mode** â€” transparent background + compact layout for streaming
- [ ] **Streaming TTS** â€” start lip sync before full audio generated
- [ ] **Spatial audio** â€” voice positioned at avatar in 3D space
- [ ] **Emotion detection** â€” analyze chat sentiment for smarter expression/action picking
- [ ] **Camera input** â€” face tracking via MediaPipe, mirror user's expressions
- [ ] **Multi-language** â€” Chinese/Japanese voice input + TTS
- [ ] **Screenshot & video export** â€” capture poses and record animation clips

## Phase 6: Dream ðŸŒŸ
- [ ] **Native mobile app** via Capacitor
- [ ] **Desktop companion** â€” transparent overlay (like Tamagotchi)
- [ ] **VR/AR mode** â€” WebXR support
- [ ] **Live streaming** â€” OBS/VTuber integration
- [ ] **Multi-character** â€” multiple VRM avatars in one scene
- [ ] **Tailscale access** â€” secure remote viewing from anywhere

## Design Principles
1. **Web-first** â€” runs in any modern browser, no install needed
2. **Offline-capable** â€” core rendering works without internet
3. **API-driven** â€” everything controllable via WebSocket
4. **Beautiful by default** â€” sakura aesthetic out of the box ðŸŒ¸
5. **Companion, not tool** â€” this is a home, not a dashboard ðŸ’£
