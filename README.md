# Clawatar ğŸ­

**From Agent Intelligence to Interactive Intelligence. Give your AI agent a body.**

A web-based 3D VRM avatar viewer with real-time animations, voice chat, and lip sync â€” built for [OpenClaw](https://github.com/openclaw/openclaw).

## Screenshots

<p align="center">
  <img src="docs/images/default-view.jpg" width="48%" alt="Default sakura theme" />
  <img src="docs/images/night-sky-face.jpg" width="48%" alt="Night sky with face close-up" />
</p>
<p align="center">
  <img src="docs/images/sakura-garden.jpg" width="48%" alt="Sakura garden with petals" />
  <img src="docs/images/sunset-fullbody.jpg" width="48%" alt="Sunset full body view" />
</p>

> *Cute sakura UI, multiple background scenes, camera presets, emotion bar, and 162 animations. VRM model not included â€” bring your own!*

## Quick Start

```bash
git clone https://github.com/Dongping-Chen/Clawatar.git
cd Clawatar
npm install
npm run start
```

Open `http://localhost:3000` and drop your `.vrm` model onto the page.

## Features

### ğŸ­ Avatar & Animation
- **162 animations** â€” wave, dance, think, laugh, shrug, and more (Mixamo VRMA)
- **Facial expressions** â€” happy, sad, angry, surprised, relaxed
- **Idle behavior** â€” avatar looks around, stretches, yawns when waiting
- **Touch reactions** â€” click the avatar for headpats, pokes, and silly reactions âœ¨

### ğŸŒ¸ Beautiful UI
- **Sakura/anime theme** â€” cute pink glassmorphism panels
- **Background scenes** â€” Sakura Garden ğŸŒ¸, Night Sky ğŸŒ™, Cozy CafÃ© â˜•, Sunset ğŸŒ…
- **Camera presets** â€” Face, Portrait, Full Body, Cinematic with smooth transitions
- **Quick emotion bar** â€” ğŸ˜ŠğŸ˜¢ğŸ˜ ğŸ˜®ğŸ˜ŒğŸ’ƒ one-tap expression + animation combos

### ğŸ¤ Voice & Chat
- **Audio-driven lip sync** â€” mouth moves to actual speech audio
- **Voice input** â€” speak via your browser's microphone
- **Voice output** â€” ElevenLabs TTS (optional, requires API key)
- **AI conversation** â€” powered by [OpenClaw](https://github.com/openclaw/openclaw) (optional)

### ğŸ  3D Scene System (Blender Pipeline)
- **6 scenes** â€” Cozy Bedroom ğŸ›ï¸, Izakaya ğŸ®, CafÃ© â˜•, Phone Booth ğŸ“, Sunset Balcony ğŸŒ‡, Swimming Pool ğŸŠ
- **Blender procedural pipeline** â€” Python scripts generate geometry + materials + lights â†’ Cycles render â†’ GLB export
- **Emissive-only materials** â€” all scenes use Emission shaders for reliable rendering in Three.js
- **Auto emissive lights** â€” brightest emissive meshes automatically spawn PointLights
- **Camera freedom** â€” orbit Â±135Â° inside scenes, configurable per-scene camera + exposure
- **Activity modes** â€” Study, Exercise, Chill with themed camera angles + animations
- **Scene loader** â€” `loadRoomGLB()` loads single GLB as entire environment with character lighting

### ğŸ“¹ Virtual Meeting Avatar
- **Join Google Meet / Zoom** â€” avatar appears via OBS Virtual Camera
- **Listen & respond** â€” captures meeting audio via BlackHole â†’ Whisper STT â†’ OpenClaw AI â†’ TTS
- **Smart triggers** â€” responds when called by name or asked a question
- **Streaming pipeline (v3)** â€” VAD + OpenClaw orchestrated model + streaming ElevenLabs TTS
- **No direct LLM calls** â€” all AI routes through OpenClaw Gateway (model selection, context, persona handled automatically)
- **Rolling context** â€” maintains 2-minute transcript window for coherent responses

### ğŸ”Œ Developer-Friendly
- **WebSocket API** â€” control everything programmatically
- **Drag & drop** â€” load any VRM model
- **Standalone mode** â€” works without OpenClaw or ElevenLabs
- **OpenClaw skill** â€” install as an agent skill for AI-driven avatars

## Bring Your Own Model

No VRM model is bundled. You can:
1. **Drag & drop** a `.vrm` file onto the viewer
2. **Set a URL** in `clawatar.config.json` â†’ `model.url`
3. **Enter a URL** in the Model panel in the UI

## Configuration

Edit `clawatar.config.json`:

```json
{
  "model": { "url": "", "autoLoad": true },
  "voice": {
    "elevenlabsVoiceId": "your-voice-id",
    "elevenlabsModel": "eleven_turbo_v2_5"
  },
  "server": { "vitePort": 3000, "wsPort": 8765, "audioPort": 8866 },
  "openclaw": { "gatewayPort": 18789, "sessionId": "vrm-chat" }
}
```

## WebSocket Protocol

```json
{"type": "play_action", "action_id": "161_Waving"}
{"type": "set_expression", "name": "happy", "weight": 0.8}
{"type": "speak", "text": "Hello!", "action_id": "161_Waving", "expression": "happy"}
{"type": "reset"}
```

## Architecture

```
Browser (localhost:3000)
â”œâ”€â”€ Three.js + @pixiv/three-vrm
â”œâ”€â”€ VRMA animation playback
â”œâ”€â”€ Audio-driven lip sync
â””â”€â”€ Chat UI + Emotion Bar
    â”‚
    â”‚ WebSocket (ws://localhost:8765)
    â–¼
WS Server (server/ws-server.ts)
â”œâ”€â”€ Command relay & routing
â”œâ”€â”€ ElevenLabs TTS
â”œâ”€â”€ OpenClaw Gateway bridge (all AI routing)
â””â”€â”€ Meeting speech â†’ Gateway API â†’ orchestrated model
    â”‚
    â–¼
OpenClaw Gateway (localhost:18789)
â”œâ”€â”€ Model orchestration (Opus/Sonnet/Codex)
â”œâ”€â”€ Session & context management
â””â”€â”€ Persona & memory
```

## OpenClaw Skill

Clawatar includes an [OpenClaw](https://github.com/openclaw/openclaw) skill at `skill/SKILL.md`. Install it to let your AI agent control the avatar with animations, expressions, and voice.

## Scripts

| Command | Description |
|---------|-------------|
| `npm run start` | Start dev server + WebSocket server |
| `npm run dev` | Vite dev server only |
| `npm run ws-server` | WebSocket server only |
| `npm run build` | Production build |
| `npm run catalog` | Regenerate animation catalog |
| `npm run meeting` | Virtual meeting bridge v2 (continuous listen + smart trigger) |
| `npm run meeting:v3` | Virtual meeting bridge v3 (streaming VAD + streaming TTS) |

## Building Scenes (Blender Pipeline)

Each scene is a Blender Python script that generates procedural geometry â†’ exports GLB.

```bash
# Build a scene
/Applications/Blender.app/Contents/MacOS/Blender --background --python blender/build_izakaya_v4.py

# Copy to public
cp /tmp/izakaya.glb public/scenes/izakaya.glb

# Load in viewer
open http://localhost:3000?room=izakaya
```

### Scene scripts (in `blender/`)
| Script | Scene | GLB Size |
|--------|-------|----------|
| `build_room_v9.py` | Cozy Bedroom | 3.7 MB |
| `build_izakaya_v4.py` | Izakaya Bar | 5.9 MB |
| `build_cafe_v6.py` | Coffee CafÃ© | 4.6 MB |
| `build_phone_booth_v6.py` | Rainy Phone Booth | 1.6 MB |
| `build_balcony_v8.py` | Sunset Balcony | 7.7 MB |
| `build_pool_v8.py` | Swimming Pool | 7.1 MB |

### Key rules for scene scripts
- **All emission strengths â‰¥ 3.0** â€” sub-1.0 gets baked dark by glTF exporter
- **Use Emission shader only** (not Principled BSDF) for reliable Three.js rendering
- **Cycles renderer** â€” 64 samples + denoiser
- **Center stage clear** â€” character stands at origin (0,0,0)
- **Background elements at Blender -Y** â€” they end up behind the character in Three.js
- **GLB under 8 MB** â€” optimize mesh complexity
- See `SCENES.md` for detailed scene configs and review scores

## Virtual Meeting Setup

1. Install [OBS Studio](https://obsproject.com/) and [BlackHole 2ch](https://existential.audio/blackhole/)
2. Create a Multi-Output Device (Audio MIDI Setup) â†’ your speakers + BlackHole 2ch
3. Set system output to the Multi-Output Device
4. OBS: Add Browser Source â†’ `http://localhost:3000?embed` â†’ Start Virtual Camera
5. Start the avatar: `npm run start`
6. Start the meeting bridge: `npm run meeting:v3`
7. In Google Meet: select OBS Virtual Camera (video) and BlackHole 2ch (mic)

See `virtual-meeting/README.md` for detailed architecture docs.

## Credits

- **Animations:** [Mixamo](https://www.mixamo.com/) â€” non-commercial use, credit required
- **VRM rendering:** [@pixiv/three-vrm](https://github.com/pixiv/three-vrm)
- **Inspired by:** [moeru-ai/airi](https://github.com/moeru-ai/airi)

## License

MIT â€” see [LICENSE](LICENSE)
