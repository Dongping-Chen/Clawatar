/**
 * Meeting Bridge v2 â€” Continuous listening with smart trigger detection.
 * 
 * Flow:
 *   1. Record 3s chunks continuously (pipelined â€” records while processing)
 *   2. Transcribe each chunk via Whisper
 *   3. Append to rolling transcript (last 2 min)
 *   4. Check triggers: name mentioned? question directed at us?
 *   5. If triggered â†’ send full context to AI â†’ TTS â†’ broadcast to VRM
 * 
 * Usage: npx tsx virtual-meeting/meeting-bridge-v2.ts
 */

import { execSync, spawn, ChildProcess } from 'child_process'
import * as fs from 'fs'
import * as path from 'path'
import WebSocket from 'ws'

// â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const WS_URL = process.env.WS_URL || 'ws://localhost:8765'
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || ''
const CHUNK_SECONDS = 3
const TRANSCRIPT_MAX_AGE_MS = 120_000 // keep 2 min of transcript
const SILENCE_THRESHOLD_DB = -50
const TMP_DIR = '/tmp/meeting-bridge'

// Trigger keywords (case-insensitive, checked against transcription)
const TRIGGER_NAMES = [
  // Reze â€” Whisper transcribes this in MANY different ways
  'reze', 'rezay', 'reh-zay', 'rezei',
  'riz', 'ruiz', 'razeh', 'razer', 'razor',
  'rezy', 'rezi', 'rezzy', 'rese', 'resay',
  'leather',  // yes, Whisper actually hears "leather" sometimes ğŸ˜‚
  'ãƒ¬ã‚¼', 'é›·æ³½', 'è•¾æ³½', 'é›·å§',
  // Dongping
  'ä¸œå¹³', 'dongping', 'dong ping',
]
// Question patterns (Chinese + English)
const QUESTION_PATTERNS = [
  /ä½ [è§‰è®¤]å¾—/,  // ä½ è§‰å¾—/ä½ è®¤ä¸º
  /[å—å˜›å‘¢][\?ï¼Ÿã€‚]?$/,  // ends with å—/å˜›/å‘¢
  /æ€ä¹ˆ[çœ‹æƒ³åŠè¯´]/,  // æ€ä¹ˆçœ‹/æ€ä¹ˆæƒ³
  /ä»€ä¹ˆæ„[è§æ€]/,  // ä»€ä¹ˆæ„è§/æ„æ€
  /å¯¹[å§ä¸][\?ï¼Ÿ]?$/,  // å¯¹å§/å¯¹ä¸å¯¹
  /æ˜¯ä¸æ˜¯/,
  /æœ‰æ²¡æœ‰/,
  /èƒ½ä¸èƒ½/,
  /å¯ä»¥.{0,4}[å—å˜›]/,
  /\?$/,  // English question mark
  /can you/i, /do you/i, /what do/i, /how do/i, /could you/i,
  /what'?s your/i, /don'?t you/i,
]

// â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
interface TranscriptEntry {
  text: string
  timestamp: number
}

let ws: WebSocket | null = null
let isRunning = true
let chunkIndex = 0
const transcript: TranscriptEntry[] = []

// Track when we last responded to avoid rapid-fire
let lastResponseTime = 0
const RESPONSE_COOLDOWN_MS = 8000

// â”€â”€â”€ WebSocket â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function connectWS(): Promise<WebSocket> {
  return new Promise((resolve, reject) => {
    const socket = new WebSocket(WS_URL)
    socket.on('open', () => {
      console.log('[bridge] Connected to WS server')
      socket.send(JSON.stringify({
        type: 'register_device',
        device_type: 'meeting-bridge',
        device_name: 'Meeting Audio Bridge v2',
      }))
      resolve(socket)
    })
    socket.on('error', reject)
    socket.on('close', () => {
      console.log('[bridge] WS disconnected, reconnecting in 3s...')
      setTimeout(() => connectWS().then(s => { ws = s }).catch(console.error), 3000)
    })
  })
}

// â”€â”€â”€ Audio Recording (sox/rec) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function recordChunk(index: number): Promise<string> {
  const rawPath = path.join(TMP_DIR, `chunk_raw_${index}.wav`)
  const outPath = path.join(TMP_DIR, `chunk_${index}.wav`)

  return new Promise((resolve, reject) => {
    const proc = spawn('rec', [
      '-q', '-r', '48000', '-c', '2', '-b', '16',
      rawPath, 'trim', '0', String(CHUNK_SECONDS),
    ], { stdio: ['pipe', 'pipe', 'pipe'] })

    let stderr = ''
    proc.stderr?.on('data', (d) => { stderr += d.toString() })

    proc.on('close', (code) => {
      if (code !== 0 || !fs.existsSync(rawPath)) {
        reject(new Error(`rec failed (code ${code}): ${stderr.slice(-200)}`))
        return
      }
      try {
        execSync(`sox "${rawPath}" -r 16000 -c 1 "${outPath}"`, { timeout: 10000 })
        fs.unlinkSync(rawPath)
        resolve(outPath)
      } catch (err) {
        reject(new Error(`downsample failed: ${err}`))
      }
    })
    proc.on('error', reject)
  })
}

// â”€â”€â”€ Speech Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function hasSpeech(wavPath: string): boolean {
  try {
    const result = execSync(
      `ffmpeg -i "${wavPath}" -af "volumedetect" -f null /dev/null 2>&1`,
      { encoding: 'utf-8', timeout: 10000 }
    )
    const meanMatch = result.match(/mean_volume:\s*([-\d.]+)\s*dB/)
    if (meanMatch) {
      return parseFloat(meanMatch[1]) > SILENCE_THRESHOLD_DB
    }
  } catch {}
  return true
}

// â”€â”€â”€ Whisper Transcription â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function transcribe(wavPath: string): Promise<string> {
  const formData = new FormData()
  const buf = fs.readFileSync(wavPath)
  formData.append('file', new Blob([buf], { type: 'audio/wav' }), 'audio.wav')
  formData.append('model', 'whisper-1')
  // Prompt helps Whisper recognize proper nouns correctly
  formData.append('prompt', 'Reze, Dongping, ä¸œå¹³, é›·æ³½')

  try {
    const resp = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}` },
      body: formData,
    })
    if (!resp.ok) return ''
    const data = await resp.json() as { text: string }
    return data.text?.trim() || ''
  } catch {
    return ''
  }
}

// â”€â”€â”€ Transcript Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function addToTranscript(text: string) {
  transcript.push({ text, timestamp: Date.now() })
  // Prune old entries
  const cutoff = Date.now() - TRANSCRIPT_MAX_AGE_MS
  while (transcript.length > 0 && transcript[0].timestamp < cutoff) {
    transcript.shift()
  }
}

function getFullTranscript(): string {
  return transcript.map(e => e.text).join(' ')
}

function getRecentTranscript(lastN: number = 5): string {
  return transcript.slice(-lastN).map(e => e.text).join(' ')
}

// â”€â”€â”€ Trigger Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
interface TriggerResult {
  triggered: boolean
  reason: string
}

function checkTrigger(latestText: string): TriggerResult {
  const lower = latestText.toLowerCase()

  // Check cooldown
  if (Date.now() - lastResponseTime < RESPONSE_COOLDOWN_MS) {
    return { triggered: false, reason: 'cooldown' }
  }

  // Check name mentions
  for (const name of TRIGGER_NAMES) {
    if (lower.includes(name.toLowerCase())) {
      return { triggered: true, reason: `name: "${name}"` }
    }
  }

  // Check question patterns (on recent transcript, not just latest chunk)
  const recent = getRecentTranscript(3).toLowerCase() + ' ' + lower
  for (const pattern of QUESTION_PATTERNS) {
    if (pattern.test(recent)) {
      // Only trigger on questions if they seem directed (contain a name or "ä½ ")
      if (lower.includes('ä½ ') || TRIGGER_NAMES.some(n => recent.includes(n.toLowerCase()))) {
        return { triggered: true, reason: `question directed at us` }
      }
    }
  }

  return { triggered: false, reason: 'no trigger' }
}

// â”€â”€â”€ AI Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function getAIResponse(context: string, latestText: string): Promise<string> {
  const prompt = `[Meeting Context â€” Rolling Transcript (last 2 min)]
${context}

[Latest Speech]
"${latestText}"

[Instructions]
You are Reze (é›·æ³½), Dongping's AI avatar in this video meeting.
- Someone just mentioned your name or asked you a question.
- Respond naturally, SHORT (1-3 sentences max).
- Use the same language as the speaker (Chinese â†’ Chinese, English â†’ English).
- You can reference earlier parts of the transcript for context.
- Be warm, helpful, and professional.
- If despite the trigger you truly have nothing to add, say [SKIP].`

  try {
    const raw = execSync(
      `openclaw agent --message ${JSON.stringify(prompt)} --json --session-id meeting-live --thinking off 2>/dev/null`,
      { encoding: 'utf-8', timeout: 30000 }
    )
    let jsonStr = '', braceDepth = 0, inJson = false
    for (const line of raw.split('\n')) {
      if (!inJson && line.trim().startsWith('{')) inJson = true
      if (inJson) {
        jsonStr += line + '\n'
        for (const ch of line) {
          if (ch === '{') braceDepth++
          if (ch === '}') braceDepth--
        }
        if (braceDepth <= 0) break
      }
    }
    if (jsonStr) {
      const parsed = JSON.parse(jsonStr)
      return parsed?.result?.payloads?.[0]?.text || ''
    }
  } catch (e: any) {
    console.error('[bridge] AI error:', e.message?.slice(0, 100))
  }
  return ''
}

// â”€â”€â”€ TTS + Broadcast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function speakResponse(text: string) {
  if (!ws || ws.readyState !== WebSocket.OPEN) {
    console.error('[bridge] Cannot speak â€” WS not connected')
    return
  }

  // Send as meeting_response â€” WS server does TTS + broadcast directly (no AI)
  ws.send(JSON.stringify({
    type: 'meeting_response',
    text,
  }))
  console.log(`  ğŸ”Š Sent to TTS`)
}

// â”€â”€â”€ Cleanup old chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function cleanupOldChunks(keepLast: number = 10) {
  const files = fs.readdirSync(TMP_DIR)
    .filter(f => f.startsWith('chunk_') && f.endsWith('.wav'))
    .sort()
  if (files.length > keepLast) {
    for (const f of files.slice(0, files.length - keepLast)) {
      try { fs.unlinkSync(path.join(TMP_DIR, f)) } catch {}
    }
  }
}

// â”€â”€â”€ Main Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function mainLoop() {
  console.log('[bridge] Starting continuous listening...')
  console.log(`[bridge] Chunk: ${CHUNK_SECONDS}s | Transcript window: ${TRANSCRIPT_MAX_AGE_MS / 1000}s`)
  console.log(`[bridge] Triggers: ${TRIGGER_NAMES.join(', ')}`)
  console.log('')

  while (isRunning) {
    const loopStart = Date.now()
    try {
      // 1. Record
      const wavPath = await recordChunk(chunkIndex++)

      // 2. Check for speech
      if (!hasSpeech(wavPath)) {
        process.stdout.write('Â·')  // silence indicator
        continue
      }

      // 3. Transcribe
      const text = await transcribe(wavPath)
      if (!text || text.length < 2) {
        process.stdout.write('Â·')
        continue
      }

      // 4. Add to rolling transcript
      addToTranscript(text)
      const elapsed = Date.now() - loopStart
      console.log(`\n[${new Date().toLocaleTimeString()}] (${elapsed}ms) "${text}"`)

      // 5. Check trigger
      const trigger = checkTrigger(text)
      if (trigger.triggered) {
        console.log(`  ğŸ¯ TRIGGERED: ${trigger.reason}`)

        // 6. Get AI response with full context
        const t = Date.now()
        const context = getFullTranscript()
        const response = await getAIResponse(context, text)
        const aiMs = Date.now() - t

        if (response && response !== '[SKIP]') {
          console.log(`  ğŸ’¬ (${aiMs}ms) "${response}"`)
          lastResponseTime = Date.now()
          await speakResponse(response)
        } else {
          console.log(`  â­ï¸ (${aiMs}ms) AI skipped`)
        }
      }

      // Cleanup
      if (chunkIndex % 20 === 0) cleanupOldChunks()

    } catch (err: any) {
      console.error(`\n[bridge] Error: ${err.message?.slice(0, 100)}`)
      await new Promise(r => setTimeout(r, 2000))
    }
  }
}

// â”€â”€â”€ Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
process.on('SIGINT', () => {
  console.log('\n[bridge] Shutting down...')
  isRunning = false
  ws?.close()
  process.exit(0)
})

async function main() {
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
  console.log('â•‘  Clawatar Meeting Bridge v2          â•‘')
  console.log('â•‘  Continuous Listen + Smart Trigger   â•‘')
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')
  console.log('')

  // Preflight
  if (!OPENAI_API_KEY) { console.error('âŒ OPENAI_API_KEY not set'); process.exit(1) }
  try { execSync('which rec', { stdio: 'pipe' }) } catch {
    console.error('âŒ sox not found (brew install sox)'); process.exit(1)
  }

  const input = execSync('SwitchAudioSource -c -t input 2>/dev/null || echo unknown', { encoding: 'utf-8' }).trim()
  console.log(`Audio input: ${input}`)
  if (!input.includes('BlackHole')) {
    console.warn('âš ï¸  Input is not BlackHole â€” set with: SwitchAudioSource -s "BlackHole 2ch" -t input')
  }

  if (!fs.existsSync(TMP_DIR)) fs.mkdirSync(TMP_DIR, { recursive: true })

  // Connect WS
  ws = await connectWS()

  // Start
  await mainLoop()
}

main().catch(console.error)
