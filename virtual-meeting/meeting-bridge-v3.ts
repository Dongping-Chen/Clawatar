/**
 * Meeting Bridge v3 â€” Streaming Pipeline for <3s latency.
 *
 * Pipeline: VAD recording â†’ Whisper STT â†’ OpenClaw (orchestrated model) â†’ ElevenLabs TTS â†’ WS broadcast
 *
 * All AI responses route through OpenClaw Gateway â€” no direct LLM API calls.
 * The Gateway handles model selection, session management, and context.
 *
 * Usage:
 *   npx tsx virtual-meeting/meeting-bridge-v3.ts          # continuous mode
 *   npx tsx virtual-meeting/meeting-bridge-v3.ts --test    # single utterance test
 */

import { execSync, spawn, ChildProcess } from 'child_process'
import * as fs from 'fs'
import * as path from 'path'
import WebSocket from 'ws'
import { randomUUID } from 'crypto'

// â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const WS_URL = process.env.WS_URL || 'ws://localhost:8765'
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || ''  // Used for Whisper STT only
const ELEVENLABS_VOICE_ID = 'L5vK1xowu0LZIPxjLSl5'
const ELEVENLABS_MODEL = 'eleven_multilingual_v2'  // turbo_v2_5 doesn't support streaming+PCM reliably
const TMP_DIR = '/tmp/meeting-bridge-v3'
const AUDIO_CACHE_DIR = path.resolve(import.meta.dirname ?? '.', '..', 'server', '_audio_cache')
const AUDIO_HTTP_PORT = 8866
const MAX_RECORDING_MS = 15_000
const TRANSCRIPT_MAX_AGE_MS = 600_000
const RESPONSE_COOLDOWN_MS = 5_000
const PROACTIVE_SILENCE_MS = 10_000
const PROACTIVE_COOLDOWN_MS = 20_000
const SPEAKER_CHANGE_PAUSE_MS = 1_800
const TEST_MODE = process.argv.includes('--test')

// â”€â”€â”€ ElevenLabs API Key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function getElevenLabsKey(): string {
  if (process.env.ELEVENLABS_API_KEY) return process.env.ELEVENLABS_API_KEY
  try {
    const cfg = JSON.parse(fs.readFileSync(path.join(process.env.HOME || '', '.openclaw', 'openclaw.json'), 'utf-8'))
    return cfg?.skills?.entries?.sag?.apiKey || ''
  } catch { return '' }
}
const ELEVENLABS_API_KEY = getElevenLabsKey()

// â”€â”€â”€ Trigger Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const TRIGGER_NAMES = [
  // â”€â”€â”€ English: correct pronunciations â”€â”€â”€
  'reze', 'rezay', 'rezei', 'reza', 'rezÃ©', 'rÃ¨ze',
  // â”€â”€â”€ English: common Whisper mishearings (Râ†’L, Râ†’W, vowel shifts) â”€â”€â”€
  'leze', 'lezay', 'lesay', 'lezy', 'lezi', 'leza', 'lezey',
  'riz', 'ruiz', 'rees', 'reese', 'reis', 'race', 'raise',
  'razeh', 'razer', 'razor', 'raser', 'raiser',
  'rezy', 'rezi', 'rezzy', 'rese', 'resay', 'reezy', 'reezay',
  'leather', 'laser', 'leaser', 'leisure', 'lesser',
  'weze', 'wezay', 'wezy',  // Wâ†’R confusion
  'rezee', 'rezzay', 'rezei', 'rezae', 'rezah',
  'lets say', 'let say',  // Whisper sometimes hears "let's say" for "Reze"
  'rz', 'rez', 'rez-ay', 'reh-zay', 'reh zay', 're zay', 're-ze',
  // â”€â”€â”€ English: phonetic fragments (partial matches) â”€â”€â”€
  'risey', 'risay', 'rizay', 'rizei', 'rizzay',
  'rezzy', 'rezi', 'rezie', 'reseh', 'resey',
  'rachel',  // surprisingly common Whisper output for "Reze"
  'raizay', 'raizei', 'razay', 'razei',
  // â”€â”€â”€ Chinese: all possible transcriptions â”€â”€â”€
  'é›·æ³½', 'è•¾æ³½', 'é›·å§', 'è•¾å§', 'è•¾æ³½',
  'ç‘æ³½', 'é”æ³½', 'èŠ®æ³½', 'è•Šæ³½', 'ç¿æ³½',
  'é›·åˆ™', 'è•¾åˆ™', 'é›·æ‹©', 'è•¾æ‹©',
  'é›·å“²', 'è•¾å“²', 'èŠ®å“²',
  'é›·è´¼', 'è•¾è´¼',  // Whisper sometimes
  'æ¥æ³½', 'æ¥åˆ™', 'æ¥å“²',  // L sound in Chinese
  'ç´¯æ³½', 'ç±»æ³½',
  'ç¤¼æ³½', 'åŠ›æ³½', 'ä¸½æ³½', 'è‰æ³½', 'åˆ©æ³½',
  'ç£Šæ³½', 'è•¾ä¸', // partial matches
  'reze', // pinyin
  // â”€â”€â”€ Chinese: Dongping's name â”€â”€â”€
  'ä¸œå¹³', 'å†¬å¹³', 'ä¸œè', 'ä¸œåª', 'å†¬è', 'ä¸œå±',
  'æ´å¹³', 'æ‡‚å¹³', 'åŠ¨å¹³',
  // â”€â”€â”€ English: Dongping mishearings â”€â”€â”€
  'dongping', 'dong ping', 'dong-ping', 'dongpin',
  'dumping', 'donping', 'tong ping', 'tongping',
  'dung ping', 'dopping', 'dong thing', 'dong king',
  // â”€â”€â”€ Japanese: ãƒ¬ã‚¼ and variations â”€â”€â”€
  'ãƒ¬ã‚¼', 'ã‚Œãœ', 'ãƒ¬ã‚¼ãƒ¼', 'ã‚Œãœãƒ¼',
  'ãƒ¬ãƒ¼ã‚¼', 'ã‚Œãƒ¼ãœ', 'ãƒ¬ã‚¤ã‚¼', 'ã‚Œã„ãœ',
  'ãƒ¬ã‚»', 'ã‚Œã›', 'ãƒ¬ã‚¸', 'ã‚Œã˜', // close sounds
  'ãƒ¬ã‚º', 'ã‚Œãš', // mishearing
  // â”€â”€â”€ Japanese: phonetic â”€â”€â”€
  'reze', 'reje', 'rese',  // romaji
  // â”€â”€â”€ Korean (just in case) â”€â”€â”€
  'ë ˆì œ', 'ë ˆì¦ˆ', 'ë ˆì„¸',
  // â”€â”€â”€ Direct address patterns (not names but signal they're talking to the avatar) â”€â”€â”€
  'hey avatar', 'hi avatar', 'hello avatar',
  'hey assistant', 'hi assistant', 'hello assistant',
  'hey ai', 'hi ai', 'hello ai',
  'hey bot', 'hi bot',
  'ä½ å¥½åŠ©æ‰‹', 'åŠ©æ‰‹ä½ å¥½', 'AIåŒå­¦', 'AIä½ å¥½',
  'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ', 'ã‚¢ãƒã‚¿ãƒ¼',
]
const QUESTION_PATTERNS = [
  // â”€â”€â”€ Chinese question patterns â”€â”€â”€
  /ä½ [è§‰è®¤]å¾—/, /[å—å˜›å‘¢ä¹ˆ][\?ï¼Ÿã€‚]?$/, /æ€ä¹ˆ[çœ‹æƒ³åŠè¯´æ ·åš]/, /ä»€ä¹ˆæ„[è§æ€]/,
  /æ˜¯æ€[ä¹ˆæ ·]/, /æ€æ ·/, /å¦‚ä½•/, /ä¸ºä»€ä¹ˆ/, /ä¸ºå•¥/, /å’‹[å›æ ·åŠ]/, /å¹²[å˜›å•¥å—]/,
  /å¯¹[å§ä¸][\?ï¼Ÿ]?$/, /æ˜¯ä¸æ˜¯/, /æœ‰æ²¡æœ‰/, /èƒ½ä¸èƒ½/, /å¯ä»¥.{0,4}[å—å˜›]/,
  /ä»€ä¹ˆæ—¶å€™/, /å“ª[é‡Œä¸ªäº›]/, /å¤š[å°‘é•¿ä¹…å¤§]/, /è°[æ˜¯æ¥çš„]?/,
  /[è¯´è®²èŠè°ˆ]ä¸€?[è¯´è®²èŠè°ˆä¸‹]/, /ä»‹ç»[ä¸€ä¸‹]*/, /è§£é‡Š[ä¸€ä¸‹]*/,
  /å¥½ä¸å¥½/, /è¡Œä¸è¡Œ/, /è¦ä¸è¦/, /æƒ³ä¸æƒ³/, /å¯¹ä¸å¯¹/,
  /çŸ¥[ä¸é“]é“/, /[äº†è§£æ˜ç™½æ¸…æ¥š].*[å—å˜›]/, /[è§‰è®¤]ä¸º/,
  /æ„[è§æ€]/, /çœ‹æ³•/, /è§‚ç‚¹/, /[å»ºæ„]è®®/, /[æƒ³çœ‹]æ³•/,
  // â”€â”€â”€ English question patterns â”€â”€â”€
  /\?$/, /\?[\s"']*$/,
  /\bcan you\b/i, /\bdo you\b/i, /\bwhat do\b/i, /\bhow do\b/i, /\bcould you\b/i,
  /\bwhat'?s your\b/i, /\bdon'?t you\b/i, /\bwould you\b/i, /\bshould we\b/i,
  /\bwhat about\b/i, /\bhow about\b/i, /\bwhat if\b/i,
  /\bdo you think\b/i, /\bwhat do you think\b/i, /\bhow do you feel\b/i,
  /\bcan someone\b/i, /\bdoes anyone\b/i, /\banyone know\b/i,
  /\btell (me|us)\b/i, /\bexplain\b/i, /\bdescribe\b/i,
  /\bwhat is\b/i, /\bwhat are\b/i, /\bwho is\b/i, /\bwhere is\b/i,
  /\bwhen (is|do|did|will|should)\b/i, /\bwhy (is|do|did|would|should)\b/i,
  /\bhow (is|do|did|would|should|can|could|many|much|long|far)\b/i,
  /\bis (it|this|that|there)\b/i, /\bare (you|we|they|there)\b/i,
  /\bthoughts\b/i, /\bopinion\b/i, /\bfeedback\b/i, /\bsuggestion\b/i,
  /\bany idea\b/i, /\bany question\b/i, /\bany comment\b/i,
  // â”€â”€â”€ Japanese question patterns â”€â”€â”€
  /[ã¾ã™ã‹ã®][\?ï¼Ÿ]?$/, /ã§ã—ã‚‡ã†ã‹/, /ã§ã™ã‹/, /ã¾ã›ã‚“ã‹/,
  /ã©ã†[æ€è€ƒ]/, /ãªãœ/, /ãªã‚“ã§/, /ã©ã†ã—ã¦/, /ä½•ãŒ/, /èª°ãŒ/, /ã„ã¤/,
  /ã©ã“/, /ã©ã‚Œ/, /ã©ã®/, /ã©ã‚“ãª/, /ã„ã‹ãŒ/,
]

// â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
interface TranscriptEntry {
  text: string
  timestamp: number
  speaker: string
}
type ResponseMode = 'triggered' | 'proactive'

const transcript: TranscriptEntry[] = []
let ws: WebSocket | null = null
let isRunning = true
let lastResponseTime = 0
let responseCooldownUntil = 0
let lastSpeechTime = 0
let isSpeaking = false  // Echo suppression: true while TTS is playing
let lastProactiveTime = 0
let inferredSpeakerIndex = 1
let currentSpeaker = `Speaker ${inferredSpeakerIndex}`

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function addToTranscript(text: string) {
  const now = Date.now()
  if (lastSpeechTime > 0 && now - lastSpeechTime >= SPEAKER_CHANGE_PAUSE_MS) {
    inferredSpeakerIndex += 1
    currentSpeaker = `Speaker ${inferredSpeakerIndex}`
  }
  transcript.push({ text, timestamp: now, speaker: currentSpeaker })
  lastSpeechTime = now
  const cutoff = Date.now() - TRANSCRIPT_MAX_AGE_MS
  while (transcript.length > 0 && transcript[0].timestamp < cutoff) transcript.shift()
}
function getFullTranscript(): string { return transcript.map(e => e.text).join(' ') }
function getRecentTranscript(n = 3): string { return transcript.slice(-n).map(e => e.text).join(' ') }
function getStructuredTranscript(): string {
  return transcript.map((e) => {
    const time = new Date(e.timestamp).toLocaleTimeString('en-US', { hour12: false })
    return `[${time}] ${e.speaker}: ${e.text}`
  }).join('\n')
}
function hasSpokenRecently(windowMs = PROACTIVE_COOLDOWN_MS): boolean {
  return Date.now() - lastResponseTime < windowMs
}
function detectArithmeticError(text: string): string | null {
  const match = text.match(/(-?\d+(?:\.\d+)?)\s*([+\-*xX])\s*(-?\d+(?:\.\d+)?)\s*=\s*(-?\d+(?:\.\d+)?)/)
  if (!match) return null
  const left = Number(match[1])
  const op = match[2]
  const right = Number(match[3])
  const claimed = Number(match[4])
  const expected = op === '+' ? left + right : op === '-' ? left - right : left * right
  if (Number.isFinite(expected) && Number.isFinite(claimed) && Math.abs(expected - claimed) > 1e-9) {
    return `${left} ${op} ${right} = ${claimed} (expected ${expected})`
  }
  return null
}

function checkTrigger(text: string): { triggered: boolean; reason: string } {
  const now = Date.now()
  const lower = text.toLowerCase()
  if (now < responseCooldownUntil) return { triggered: false, reason: 'cooldown' }

  // 1. Name match â€” highest priority
  for (const name of TRIGGER_NAMES) {
    if (lower.includes(name.toLowerCase())) return { triggered: true, reason: `name: "${name}"` }
  }

  // 2. Direct commands / requests to the avatar (even without name)
  const directCommandPatterns = [
    /è‡ªæˆ‘ä»‹ç»/, /ä»‹ç»[ä¸€ä¸‹]*ä½ /, /ä½ .{0,4}ä»‹ç»/, /ä½ .{0,4}è¯´[ä¸€ä¸‹è¯´]/, 
    /ä½ èƒ½åšä»€ä¹ˆ/, /ä½ ä¼šä»€ä¹ˆ/, /ä½ [æ˜¯åš]ä»€ä¹ˆ/, /ä½ çš„èƒŒæ™¯/, /ä½ çš„åŠŸèƒ½/, /ä½ çš„èƒ½åŠ›/,
    /tell (me|us) about (yourself|you)/i, /introduce yourself/i, /what can you do/i,
    /what are you/i, /who are you/i, /describe yourself/i,
    /è‡ªå·±ç´¹ä»‹/, /ã‚ãªãŸã¯èª°/, /ä½•ãŒã§ãã‚‹/,
    /éœ€è¦ä½ /, /è¯·ä½ /, /å¸®æˆ‘/, /ç»™æˆ‘/,  // requests directed at "you"
    /\byour (background|ability|feature|function|capability)\b/i,
  ]
  for (const pat of directCommandPatterns) {
    if (pat.test(text)) return { triggered: true, reason: 'direct command/request' }
  }

  // 3. Question + "ä½ " (directed at us) in recent context
  const recent = getRecentTranscript(3).toLowerCase() + ' ' + lower
  for (const pat of QUESTION_PATTERNS) {
    if (pat.test(recent) && (lower.includes('ä½ ') || TRIGGER_NAMES.some(n => recent.includes(n.toLowerCase())))) {
      return { triggered: true, reason: 'question directed at us' }
    }
  }

  // 4. Group questions (anyone / everyone / team)
  const groupQuestionPatterns = [
    /\b(anyone|someone|team|folks|everyone|any thoughts|what do you all think)\b/i,
    /å¤§å®¶[è§‰å¾—æ€ä¹ˆçœ‹æƒ³æ³•]/, /æœ‰äººçŸ¥é“/, /æˆ‘ä»¬[è§‰å¾—æ€ä¹ˆçœ‹æƒ³æ³•]/,
    /è°[èƒ½ä¼šå¯]/, /å“ªä½/,
  ]
  const directedElsewherePatterns = [
    /@\w+/,
    /\b(can you|could you|what do you think),?\s+(john|mike|sarah|professor|è€å¸ˆ|åŒå­¦)\b/i,
  ]
  const looksLikeQuestion = QUESTION_PATTERNS.some((p) => p.test(lower))
  const directedElsewhere = directedElsewherePatterns.some((p) => p.test(text))
  const asksGroup = groupQuestionPatterns.some((p) => p.test(text))
  // Only trigger on explicit group questions (å¤§å®¶/anyone/everyone), NOT general questions
  if (asksGroup && !hasSpokenRecently()) {
    return { triggered: true, reason: 'group question' }
  }

  // 5. Factual correction
  const arithmeticError = detectArithmeticError(text)
  if (arithmeticError && !hasSpokenRecently(10_000)) {
    return { triggered: true, reason: `polite factual correction (${arithmeticError})` }
  }
  return { triggered: false, reason: 'no trigger' }
}

function pickAction(text: string): { action_id: string; expression: string; expression_weight: number } {
  const l = text.toLowerCase()
  if (l.match(/\b(haha|lol|funny|laugh|ğŸ˜‚)\b/)) return { action_id: '125_Laughing', expression: 'happy', expression_weight: 0.9 }
  if (l.match(/\b(hi|hello|hey|ä½ å¥½|å—¨)\b/)) return { action_id: '161_Waving', expression: 'happy', expression_weight: 0.7 }
  if (l.match(/\b(yes|yeah|sure|å¥½çš„|å¯¹|æ˜¯çš„)\b/)) return { action_id: '118_Head Nod Yes', expression: 'happy', expression_weight: 0.6 }
  if (l.match(/\b(no|nope|ä¸|æ²¡æœ‰)\b/)) return { action_id: '144_Shaking Head No', expression: 'neutral', expression_weight: 0.5 }
  if (l.match(/\b(think|hmm|æƒ³|å¯èƒ½)\b/)) return { action_id: '88_Thinking', expression: 'neutral', expression_weight: 0.5 }
  if (l.match(/\b(thank|thanks|è°¢è°¢)\b/)) return { action_id: '156_Thankful', expression: 'happy', expression_weight: 0.8 }
  return { action_id: '86_Talking', expression: 'happy', expression_weight: 0.5 }
}

// â”€â”€â”€ Play audio to BlackHole 16ch (meeting participants hear avatar) â”€â”€â”€
async function playToBlackHole(audioUrl: string) {
  try {
    const devices = execSync('SwitchAudioSource -a 2>/dev/null || true', { encoding: 'utf-8' })
    if (!devices.includes('BlackHole 16ch')) {
      console.log('[v3] âš ï¸ BlackHole 16ch not available')
      return
    }
    // Download audio file and play through BlackHole 16ch
    const tmpFile = path.join(TMP_DIR, `bh_${Date.now()}.wav`)
    const resp = await fetch(audioUrl)
    if (!resp.ok) throw new Error(`Fetch failed: ${resp.status}`)
    const buf = Buffer.from(await resp.arrayBuffer())
    fs.writeFileSync(tmpFile, buf)
    
    // Echo suppression: mark as speaking so VAD ignores our own voice
    isSpeaking = true
    console.log('[v3] ğŸ”‡ Echo suppression ON')
    
    const player = spawn('play', ['-q', tmpFile], {
      env: { ...process.env, AUDIODEV: 'BlackHole 16ch' },
      stdio: 'ignore',
    })
    player.on('close', () => {
      // Add extra 1.5s buffer after playback ends to catch tail echo
      setTimeout(() => {
        isSpeaking = false
        console.log('[v3] ğŸ”Š Echo suppression OFF')
      }, 1500)
      try { fs.unlinkSync(tmpFile) } catch {}
    })
    player.on('error', (e) => {
      isSpeaking = false
      console.error(`[v3] BlackHole play error: ${e.message}`)
    })
  } catch (e: any) {
    isSpeaking = false
    console.error(`[v3] playToBlackHole error: ${e.message}`)
  }
}

// â”€â”€â”€ WebSocket â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function connectWS(): Promise<WebSocket> {
  return new Promise((resolve, reject) => {
    const socket = new WebSocket(WS_URL)
    socket.on('open', () => {
      console.log('[v3] Connected to WS server')
      socket.send(JSON.stringify({ type: 'register_device', device_type: 'meeting-bridge', device_name: 'Meeting Bridge v3' }))
      resolve(socket)
    })
    socket.on('error', reject)
    socket.on('close', () => {
      if (!isRunning) return
      console.log('[v3] WS disconnected, reconnecting in 3s...')
      setTimeout(() => connectWS().then(s => { ws = s }).catch(console.error), 3000)
    })
  })
}

// â”€â”€â”€ 1. VAD Recording â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function recordVAD(): Promise<{ wavPath: string; durationMs: number }> {
  return new Promise((resolve, reject) => {
    const id = randomUUID().slice(0, 8)
    const rawPath = path.join(TMP_DIR, `vad_raw_${id}.wav`)
    const outPath = path.join(TMP_DIR, `vad_${id}.wav`)
    const startTime = Date.now()

    // sox silence: start when >1% for 0.1s, stop after 1.5s of <1%
    const proc = spawn('/opt/homebrew/bin/rec', [
      '-q', '-r', '48000', '-c', '2', '-b', '16',
      rawPath,
      'silence', '1', '0.1', '1%', '1', '1.0', '1%',
    ], { stdio: ['pipe', 'pipe', 'pipe'] })

    let killed = false
    // Max duration cap
    const timer = setTimeout(() => {
      killed = true
      proc.kill('SIGTERM')
    }, MAX_RECORDING_MS)

    proc.on('close', (code) => {
      clearTimeout(timer)
      const durationMs = Date.now() - startTime
      if (!fs.existsSync(rawPath)) {
        if (killed) {
          // max duration reached, rawPath might still exist
        }
        reject(new Error(`rec produced no output (code ${code})`))
        return
      }
      try {
        // Downsample to 16kHz mono for Whisper
        execSync(`sox "${rawPath}" -r 16000 -c 1 "${outPath}"`, { timeout: 10_000 })
        fs.unlinkSync(rawPath)
        resolve({ wavPath: outPath, durationMs })
      } catch (err) {
        reject(new Error(`downsample failed: ${err}`))
      }
    })
    proc.on('error', reject)
  })
}

// â”€â”€â”€ 2. Whisper STT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function transcribeWhisper(wavPath: string): Promise<string> {
  // Check for empty audio (BlackHole with no input)
  const stat = fs.statSync(wavPath)
  if (stat.size <= 44) {  // WAV header only = no audio data
    console.log('[v3] âš ï¸ Empty audio file, skipping Whisper')
    return ''
  }
  const formData = new FormData()
  formData.append('file', new Blob([fs.readFileSync(wavPath)], { type: 'audio/wav' }), 'audio.wav')
  formData.append('model', 'whisper-1')
  formData.append('prompt', 'Reze, Dongping, ä¸œå¹³, é›·æ³½')

  const resp = await fetch('https://api.openai.com/v1/audio/transcriptions', {
    method: 'POST',
    headers: { Authorization: `Bearer ${OPENAI_API_KEY}` },
    body: formData,
  })
  if (!resp.ok) throw new Error(`Whisper error: ${resp.status}`)
  const data = await resp.json() as { text: string }
  return data.text?.trim() || ''
}

// AI responses are handled entirely by OpenClaw Gateway (via WS â†’ meeting_speech â†’ Gateway API).
// No direct LLM API calls â€” the Gateway handles model selection, context, and persona.

// â”€â”€â”€ 4. ElevenLabs Streaming TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
interface TTSResult {
  audioUrl: string
  firstChunkMs: number
  wavPath?: string  // uncompressed WAV for virtual mic output
}

async function streamTTS(textChunks: AsyncIterable<string>): Promise<TTSResult> {
  return new Promise(async (resolve, reject) => {
    const audioBuffers: Buffer[] = []  // collect for WAV file + VRM
    let firstChunkTime: number | null = null
    const startTime = Date.now()
    let resolved = false

    // Start sox player: reads MP3 from stdin â†’ plays to BlackHole 16ch IMMEDIATELY
    let soxPlayer: ChildProcess | null = null
    try {
      const devices = execSync('SwitchAudioSource -a 2>/dev/null || true', { encoding: 'utf-8' })
      if (devices.includes('BlackHole 16ch')) {
        soxPlayer = spawn('play', [
          '-q', '-t', 'mp3', '-',  // MP3 streaming input
        ], {
          env: { ...process.env, AUDIODEV: 'BlackHole 16ch' },
          stdio: ['pipe', 'ignore', 'ignore'],
        })
        soxPlayer.on('error', (e) => console.error(`[v3] sox player error: ${e.message}`))
      }
    } catch {}

    const wsUrl = `wss://api.elevenlabs.io/v1/text-to-speech/${ELEVENLABS_VOICE_ID}/stream-input?model_id=${ELEVENLABS_MODEL}&output_format=mp3_44100_128`  // PCM requires Pro tier
    const elWs = new WebSocket(wsUrl)

    elWs.on('open', async () => {
      elWs.send(JSON.stringify({
        text: ' ',
        voice_settings: { stability: 0.45, similarity_boost: 0.75 },
        xi_api_key: ELEVENLABS_API_KEY,
      }))

      for await (const chunk of textChunks) {
        if (elWs.readyState === WebSocket.OPEN) {
          elWs.send(JSON.stringify({ text: chunk }))
        }
      }

      if (elWs.readyState === WebSocket.OPEN) {
        elWs.send(JSON.stringify({ text: '' }))
      }
    })

    elWs.on('message', (data) => {
      try {
        const msg = JSON.parse(data.toString())
        if (msg.audio) {
          const buf = Buffer.from(msg.audio, 'base64')
          audioBuffers.push(buf)

          if (!firstChunkTime) {
            firstChunkTime = Date.now()
            console.log(`[v3] ğŸ”Š First audio chunk! (${firstChunkTime - startTime}ms)`)
          }

          // REAL STREAMING: pipe PCM to sox â†’ BlackHole 16ch (plays IMMEDIATELY)
          if (soxPlayer?.stdin?.writable) {
            soxPlayer.stdin.write(buf)
          }
        }
        if (msg.isFinal) {
          elWs.close()
        }
      } catch {}
    })

    elWs.on('close', () => {
      if (resolved) return
      resolved = true
      if (soxPlayer?.stdin) soxPlayer.stdin.end()

      if (audioBuffers.length === 0) {
        reject(new Error('No audio received from ElevenLabs'))
        return
      }

      // Save WAV for VRM viewer (browser needs a file URL)
      const combined = Buffer.concat(audioBuffers)
      const wavFileName = `${randomUUID()}.wav`
      fs.mkdirSync(AUDIO_CACHE_DIR, { recursive: true })

      const wavHeader = Buffer.alloc(44)
      const dataSize = combined.length
      wavHeader.write('RIFF', 0)
      wavHeader.writeUInt32LE(dataSize + 36, 4)
      wavHeader.write('WAVE', 8)
      wavHeader.write('fmt ', 12)
      wavHeader.writeUInt32LE(16, 16)
      wavHeader.writeUInt16LE(1, 20)
      wavHeader.writeUInt16LE(1, 22)
      wavHeader.writeUInt32LE(44100, 24)
      wavHeader.writeUInt32LE(88200, 28)
      wavHeader.writeUInt16LE(2, 32)
      wavHeader.writeUInt16LE(16, 34)
      wavHeader.write('data', 36)
      wavHeader.writeUInt32LE(dataSize, 40)

      const wavPath = path.join(AUDIO_CACHE_DIR, wavFileName)
      fs.writeFileSync(wavPath, Buffer.concat([wavHeader, combined]))

      const audioUrl = `http://localhost:${AUDIO_HTTP_PORT}/audio/${wavFileName}`
      resolve({ audioUrl, firstChunkMs: (firstChunkTime || Date.now()) - startTime, wavPath })
    })

    elWs.on('error', (err) => {
      if (soxPlayer?.stdin) soxPlayer.stdin.end()
      if (!resolved) { resolved = true; reject(err) }
    })
  })
}


// â”€â”€â”€ Sentence Splitter for Streaming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Accumulates tokens and yields complete sentences
async function* sentenceSplitter(tokens: AsyncGenerator<string>): AsyncGenerator<string> {
  let buffer = ''
  const sentenceEnders = /[ã€‚ï¼ï¼Ÿ.!?\n]/

  for await (const token of tokens) {
    buffer += token
    // Check if buffer contains a sentence boundary
    const match = buffer.match(sentenceEnders)
    if (match && match.index !== undefined) {
      const idx = match.index + 1
      const sentence = buffer.slice(0, idx).trim()
      buffer = buffer.slice(idx)
      if (sentence) yield sentence + ' '
    }
  }
  // Flush remaining
  if (buffer.trim()) yield buffer.trim()
}

/**
 * Send triggered/proactive speech to WS server â†’ OpenClaw main session (full context).
 * WS server handles TTS + broadcast. Bridge listens for speak_audio back â†’ plays to BlackHole.
 */
async function runResponse(mode: ResponseMode, latestText: string, triggerReason: string, timings: Record<string, number>, t0: number) {
  if (!ws || ws.readyState !== WebSocket.OPEN) {
    console.log('[v3] âš ï¸ WS not connected, cannot send to OpenClaw')
    return null
  }

  const sendTime = Date.now()
  const transcriptContext = getStructuredTranscript()

  // Send to WS server as meeting_speech â€” WS server routes to OpenClaw main session
  ws.send(JSON.stringify({
    type: 'meeting_speech',
    text: latestText,
    transcript: transcriptContext,
    reason: triggerReason,
    mode,
  }))

  console.log(`[v3] ğŸ“¤ Sent to OpenClaw (${mode}): "${latestText.slice(0, 60)}..."`)

  // Wait for speak_audio response from WS server (OpenClaw â†’ TTS â†’ broadcast â†’ back to us)
  const responsePromise = new Promise<string | null>((resolve) => {
    const timeout = setTimeout(() => {
      ws?.removeListener('message', handler)
      console.log('[v3] â° Response timeout (60s)')
      resolve(null)
    }, 60_000)

    function handler(data: any) {
      try {
        const msg = JSON.parse(data.toString())
        if (msg.type === 'speak_audio' && msg.audio_url) {
          clearTimeout(timeout)
          ws?.removeListener('message', handler)
          console.log(`[v3] ğŸ”Š Got response: "${(msg.text || '').slice(0, 60)}..."`)
          
          // Play audio to BlackHole 16ch so meeting participants hear it
          playToBlackHole(msg.audio_url).catch(e => console.error(`[v3] BlackHole play error: ${e.message}`))
          
          resolve(msg.text || '')
        }
      } catch {}
    }
    ws?.on('message', handler)
  })

  const responseText = await responsePromise
  
  timings.total = (Date.now() - t0) / 1000
  timings.roundTrip = (Date.now() - sendTime) / 1000

  if (!responseText) {
    console.log(`[v3] â­ï¸  No response (${mode})`)
    if (mode === 'proactive') lastProactiveTime = Date.now()
    return null
  }

  const now = Date.now()
  lastResponseTime = now
  responseCooldownUntil = now + (mode === 'proactive' ? PROACTIVE_COOLDOWN_MS : RESPONSE_COOLDOWN_MS)
  if (mode === 'proactive') lastProactiveTime = now

  console.log(
    `[v3] â±ï¸  VAD:${timings.vad.toFixed(1)}s STT:${timings.stt.toFixed(1)}s ` +
    `RT:${timings.roundTrip.toFixed(1)}s TOTAL:${timings.total.toFixed(1)}s`
  )

  return responseText
}

// â”€â”€â”€ Full Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function processUtterance(): Promise<{
  text: string
  response: string
  mode: ResponseMode
  timings: Record<string, number>
} | null> {
  const timings: Record<string, number> = {}
  const t0 = Date.now()

  // 0. Echo suppression â€” skip if we're currently playing TTS
  if (isSpeaking) {
    console.log('[v3] ğŸ”‡ Skipping (echo suppression â€” TTS playing)')
    await new Promise(r => setTimeout(r, 1000))
    return null
  }

  // 1. VAD Record
  console.log('[v3] ğŸ¤ Listening (VAD)...')
  const { wavPath, durationMs: vadMs } = await recordVAD()
  timings.vad = vadMs / 1000
  console.log(`[v3] VAD done: ${(vadMs / 1000).toFixed(1)}s`)

  // Echo suppression â€” if we started speaking during VAD recording, discard
  if (isSpeaking) {
    console.log('[v3] ğŸ”‡ Discarding (TTS started during recording)')
    try { fs.unlinkSync(wavPath) } catch {}
    return null
  }

  // 2. STT
  const sttStart = Date.now()
  const text = await transcribeWhisper(wavPath)
  timings.stt = (Date.now() - sttStart) / 1000

  // Cleanup wav
  try { fs.unlinkSync(wavPath) } catch {}

  if (!text || text.length < 2) {
    const now = Date.now()
    const silenceMs = lastSpeechTime > 0 ? now - lastSpeechTime : 0
    const transcriptChars = getFullTranscript().length
    const proactiveEligible =
      transcriptChars > 200 &&
      silenceMs >= PROACTIVE_SILENCE_MS &&
      now >= responseCooldownUntil &&
      (lastProactiveTime === 0 || now - lastProactiveTime >= PROACTIVE_COOLDOWN_MS)

    if (proactiveEligible) {
      console.log(`[v3] ğŸ¤« Silence ${silenceMs}ms, proactive contribution...`)
      const response = await runResponse('proactive', '', `silence>${PROACTIVE_SILENCE_MS}ms`, timings, t0)
      if (response) return { text: '', response, mode: 'proactive', timings }
      return null
    }
    process.stdout.write('Â·')
    return null
  }

  console.log(`[v3] ğŸ“ "${text}" (STT: ${timings.stt.toFixed(1)}s)`)
  addToTranscript(text)

  // 3. Check trigger
  const trigger = checkTrigger(text)
  if (!trigger.triggered) {
    console.log(`[v3] â­ï¸  No trigger (${trigger.reason})`)
    return null
  }
  console.log(`[v3] ğŸ¯ TRIGGERED: ${trigger.reason}`)

  const response = await runResponse('triggered', text, trigger.reason, timings, t0)
  if (!response) return null
  return { text, response, mode: 'triggered', timings }
}

// â”€â”€â”€ Main Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function mainLoop() {
  console.log('[v3] Starting continuous VAD listening...')
  while (isRunning) {
    try {
      await processUtterance()
    } catch (err: any) {
      console.error(`[v3] Error: ${err.message?.slice(0, 150)}`)
      await new Promise(r => setTimeout(r, 2000))
    }
  }
}

// â”€â”€â”€ Test Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function testMode() {
  console.log('[v3] TEST MODE â€” recording one utterance, routing through OpenClaw...\n')

  if (!ws || ws.readyState !== WebSocket.OPEN) {
    console.error('âŒ WS not connected â€” OpenClaw routing requires WS server')
    process.exit(1)
  }

  // Force trigger by temporarily disabling cooldown
  lastResponseTime = 0
  responseCooldownUntil = 0

  // Record + transcribe
  const t0 = Date.now()
  console.log('[v3] ğŸ¤ Speak now (VAD will detect when you stop)...')
  const { wavPath, durationMs } = await recordVAD()
  console.log(`[v3] VAD: ${(durationMs / 1000).toFixed(1)}s`)

  const sttStart = Date.now()
  const text = await transcribeWhisper(wavPath)
  const sttMs = Date.now() - sttStart
  console.log(`[v3] STT (${(sttMs / 1000).toFixed(1)}s): "${text}"`)
  try { fs.unlinkSync(wavPath) } catch {}

  if (!text) { console.log('[v3] No speech detected.'); return }

  addToTranscript(text)

  // Route through OpenClaw via WS server (same path as production)
  const timings: Record<string, number> = { vad: durationMs / 1000, stt: sttMs / 1000 }
  const response = await runResponse('triggered', text, 'test mode', timings, t0)

  const totalMs = Date.now() - t0
  console.log(`\n[v3] â±ï¸  TIMINGS:`)
  console.log(`  VAD:        ${(durationMs / 1000).toFixed(2)}s`)
  console.log(`  STT:        ${(sttMs / 1000).toFixed(2)}s`)
  console.log(`  Round-trip: ${(timings.roundTrip || 0).toFixed(2)}s`)
  console.log(`  TOTAL:      ${(totalMs / 1000).toFixed(2)}s`)

  if (response) {
    console.log(`[v3] âœ… Response: "${response.slice(0, 100)}..."`)
  } else {
    console.log('[v3] â­ï¸ No response')
  }
}

// â”€â”€â”€ Entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
process.on('SIGINT', () => { console.log('\n[v3] Shutting down...'); isRunning = false; ws?.close(); process.exit(0) })

async function main() {
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
  console.log('â•‘  Clawatar Meeting Bridge v3          â•‘')
  console.log('â•‘  Streaming Pipeline (<3s target)     â•‘')
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

  if (!OPENAI_API_KEY) { console.warn('âš ï¸  OPENAI_API_KEY not set â€” Whisper STT will fail. Set it for speech transcription.') }
  if (!ELEVENLABS_API_KEY) { console.error('âŒ ElevenLabs API key not found'); process.exit(1) }

  try { execSync('which rec', { stdio: 'pipe' }) } catch {
    console.error('âŒ sox not found (brew install sox)'); process.exit(1)
  }

  const input = execSync('SwitchAudioSource -c -t input 2>/dev/null || echo unknown', { encoding: 'utf-8' }).trim()
  console.log(`Audio input: ${input}`)
  if (!input.includes('BlackHole')) {
    console.warn('âš ï¸  Input is not BlackHole â€” set with: SwitchAudioSource -s "BlackHole 2ch" -t input')
  }

  fs.mkdirSync(TMP_DIR, { recursive: true })

  // Connect WS (non-blocking for test mode)
  try { ws = await connectWS() } catch (e) { console.warn('[v3] WS not available, continuing without broadcast') }

  if (TEST_MODE) {
    await testMode()
    process.exit(0)
  } else {
    await mainLoop()
  }
}

main().catch(console.error)
